# Peft-Lora
Using Peft-Lora to finetune a 7B parameter LLM

Using HF Peft quick tour as reference (https://huggingface.co/docs/peft/quicktour), we create a LLM fine-tuned on the task of predicitng tags based on quotes provided by English Authors. 

Using Peft-Lora config we wrap this config with our base model to generate a Lora checkpoint which only stores the weights generated by PEFT and has a file size of 30MB, greatly reducing memory required for checkpointing. 
